# OGC API - Features
The architecture of the Features implementation is structured into modules. The modular approach enables different types of data backends to be easily integrated in a similar way. Within this task, connectivityâ€™s for OGC Web Feature Service (WFS) and [Elasticsearch](https://www.elastic.co) backends were implemented.

## Core Architecture
- *controller:* Controller classes accept API requests, choose the correct backend to handle a request and return the result to the client. The controller classes (empty stubs) are generated by the OpenAPI code generator along with the model classes. 
- *request transformer:* A request transformer is specific to the backend type. Request transformers convert a request to the Features API into a corresponding request to the specific data backend. For example a request to the _/items_ endpoint is converted into a WFS _getFeature_ request.
- *query transformer:* A query transformer is a backend specific implementation that translates query parameters into the domain specific language of the backend. For example a query transformer translates the datetime filter parameter of the _/items_ endpoint into a WFS filter expression.
- *format transformer:* A format transfomer is optionally interposed if a data backend has no support for the geoJSON format
- *backend:* A backend stores the configuration for a instance of a backend type (e.g URL of a WFS instance) and determines which type of request transformer has to be used for that specific backend type

![architecture overview](https://raw.githubusercontent.com/opengeospatial/T17-API-D165/main/resources/diagrams/d165_api_features_architecture.png "schematic representation of the architecture of the OGC API - Features implementation with two data backends")

## Data Backends

### OGC Web Feature Service Backend
The WFS backend can be used to create an OGC API - Features wrapper for an arbitrary WFS (2.x) instance. A request to the _/items_ endpoint is converted to a _getFeature_ WFS request. The metadata for the collection endpoints of Features is automatically obtained and parsed from the WFS capabilities document.

### Elasticsearch Backend
The Elasticsearch backend implementation is mainly meant to be used with the [OGC API - Records](https://ogcapi.ogc.org/records/) GeoJSON features that were generated and indexed in the [D168 Data Backend and Deployment task](https://github.com/opengeospatial/T17-API-D168) of the API Experiments thread of OGC Testbed 17. With the Elasticsearch backend the Features implementation can be used as an OGC API - Records implementation at the same time since the API endpoints are identical. The Elasticsearch backend is able to connect to Amazon Web Service (AWS) Elasticsearch Service/OpenSearch Service instances. Therefore the [Elasticsearch Python package version _7.13.4_ must be used](https://gitlab.ogc.org/ogc/T17-D040-API-Experiments-ER/-/blob/master/ER/arc_comp_d168.adoc#user-content-catalogs-deployed-via-amazon-opensearch-previously-called-elasticsearch-service) because newer versions of the package do not connect to the AWS flavor of Elasticsearch.

In general the Elasticsearch backend is able to serve generic GeoJSON features indexed in an Elasticsearch index. An Elasticsearch index corresponds to a collection. The assumption is made that the GeoJSON features match the general structure of the OGC API - Records definitions in order to implement the filter capabilities (e.g datetime filter) of OGC API - Features.
## Deployment
This section contains information on how to deploy the python server implementation.
### Run locally
The application can be started locally using the build-in Flask webserver. This is not meant for use in a production environment and should only be used for development and debugging. Before running the application all required python packages must be installed (defined in *requirements.txt*). Using a virtual python environment (like VENV) is helpful for managing the required packages.  
Start the the application with the following command:  
`python -m openapi_server.main.py`
### Run with Docker
The easiest way to deploy and run the application is using Docker. The dockerfile is structured in such a way that all required dependencies are defined in it. It is also structured as a multistage build comprising targets for production deployment and debugging.
For production deplyoment use the following command to build the docker image:  
`docker build -t tb17_apiexperiments_featuresserver_python --target prod .`  
The resulting image uses production-ready webserver.  
For debugging purposes use the following command to build the docker image:  
`docker build -t tb17_apiexperiments_featuresserver_python --target debug .`  
The resulitng image uses the built-in Flask server and has additional debugging dependencies installed. 
To start a docker container run the following command:  
`docker run -p 8080:8080 tb17_apiexperiments_featuressserver_python`
#### Debugging with Docker and Visual Studio Code
The repository includes a launch configuration (*.vscode/launch.json*) for the open-source IDE Visual Studio Code that can be used for debugging while running the application in a docker container.  
First start the application with Docker Compose:  
`docker-compose up --profile debug`.  
Second, simply start the debugger in Visual Studio Code (*Run -> Start Debugging*). After that it is possible to debug the application within Visual Studio Code (e.g. defining break points) as if it was running locally without Docker.
#### Configuration
The default backend configuration can be overridden by binding a local configuration file to the docker container. This can be done by adding a volume binding to the above Docker run command.  
`-v /path/to/backend_configuration.json:/usr/src/app/backend_configuration.json`  
The default configuration is equal to the example configuration file below.  

##### OGC WFS backend
The OGC API - Feature implementation currently uses OGC WFS implementations as data backends. The data backends are defined in a configuration (JSON) file.  
- _type_ attribute must be "WFS" for OGC WFS backends
- _collections_ attribute contains a list of WFS feature types that should be served through OGC API Features
  - feature type name equals collection name
- the _type_ attribute contains feature type/collection specific configuration
  - _temporalProperty_ specifies the property that is considered for datetime filter requests
##### Elasticsearch backend
The OGC API - Feature implementation is able to serve geojson features that are indexed in an elasticsearch index.  The Elasticsearch backend is primarily made to serve OGC API records features. Though generic geojson features could be served, datetime format must be the same as in OGC API - Records.  
- _type_ attribute must be "Elasticsearch" for elasticsearch backends
- collections attribute contains a list of elasticsearch indexes that should be made available through OGC API Features
  - index name equals collection name
- the type attribute contains index/collection specific configuration
  - _temporalProperty_ specifies the property that is considered for datetime filter requests
  - _description_, _title_ and _bbox_ attributes define collection metadata that is provided through the _/collections_ endpoint of OGC API - Features

###### AWSAuth
_awsAuth_ property must be defined if the elasticsearch instances is actually an AWS Elasticsearch Service instance.  
Authentication is handled by boto3 (AWS client libbray for python). Credentials can be either provided in a [credentials file](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#shared-credentials-file) or as [environment variables](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#environment-variables) (inside the docker container).  
  
In order to use a credentials file the file can be mounted into the docker container:  
`-v path/to/.aws/credentials:/root/.aws/credentials:ro`  
The host path depends on the operating system of the host system and user specific settings.  
  
Alternatively, credentials can be passed as [environment variables in the docker run command](https://docs.docker.com/engine/reference/commandline/run/#set-environment-variables--e---env---env-file). At least _AWS_ACCESS_KEY_ID_ and _AWS_SECRET_ACCESS_KEY_ environment variables must be passed in the run command.  
  
- _region_ attribute specifies the AWS region in which the elasticsearch service instance is hosted
- (optional) _profile_ attribute: only needed if credentials are provided by credentials file an profile is not _default_.
  
**Example Backend Configuration**
<details>
<summary>backend_configuration.json</summary>
<p>

```json
{
	"server": {
		"title": "TB-17 Experiments API Python Server",
		"description": "TB-17 Experiments API Python Server"
	},
	"backends": [
		{
	    "id": "cuberworxWFS_Foundation",
            "type": "WFS",
	    "config": {
				"baseURL": "https://test.cubewerx.com/cubewerx/cubeserv/demo?datastore=Foundation",
				"types": {
					"cw:coastl_1m": {
						"temporalProperty": null
					}
				}
			},
			"collections": [
				"cw:coastl_1m"
			]
		},
		{
	    "id": "api_records_collection",
            "type": "Elasticsearch",
			"config": {
				"baseURL": "my-es-service.eu-west-2.es.amazonaws.com",
				"port": 443,
				"useSSL": true,
				"types": {
					"record-index": {
						"temporalProperty": "extents.temporal.interval",
						"description": "a collection of api records items",
						"title": "api records",
						"bbox": [-180.0,-90.0,180.0,90.0]
					}
				},
				"awsAuth": {
					"region": "eu-west-2"
				}
			},
			"collections": [
				"record-index"
			]
		}
	]
}
```

</p>
</details>